{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、线性回归\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig0.png\" width=300>\n",
    "\n",
    "我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。  \n",
    "我们记输入为$X \\in \\mathbb{R}^{n \\times m}$，输出为$Z \\in \\mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。  \n",
    "输入层到输出层的权重和偏置，我们记为$W \\in \\mathbb{R}^{m}$和$b \\in \\mathbb{R}$。  \n",
    "输出层没有激活函数，所以上面的神经网络的前向传播过程写为：\n",
    "\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "我们使用均方误差作为模型的损失函数\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "我们通过调整参数$W$和$b$来降低均方误差，或者说是以降低均方误差为目标，学习参数$W$和参数$b$。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。\n",
    "\n",
    "在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。\n",
    "\n",
    "在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数：\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。\n",
    "\n",
    "接下来我们会求解损失函数对参数的偏导数。\n",
    "\n",
    "损失函数MSE记为：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$\n",
    "\n",
    "其中，$Z \\in \\mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。\n",
    "\n",
    "我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘：\n",
    "\n",
    "**这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\\n",
    "&= - \\frac{2}{n} X^\\mathrm{T} (y - Z)\\\\\n",
    "&= \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "\\end{aligned}$$\n",
    "\n",
    "同理，求解损失函数对参数$b$的偏导数:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\\n",
    "&= - \\frac{2}{n} \\sum^n_{i=1}(y_i - Z_i)\\\\\n",
    "&= \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。**\n",
    "\n",
    "其中，$\\frac{\\partial \\mathrm{loss}}{\\partial W} \\in \\mathbb{R}^{m}$，$\\frac{\\partial \\mathrm{loss}}{\\partial b} \\in \\mathbb{R}$，求解得到的梯度的维度与参数一致。\n",
    "\n",
    "完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、对数几率回归\n",
    "只有一个输入层和一个输出层，还有一个激活函数，$\\rm sigmoid$，简记为$\\sigma$。  \n",
    "我们设输入为$X \\in \\mathbb{R}^{n \\times m}$，输入层到输出层的权重为$W \\in \\mathbb{R}^{m}$，偏置$b \\in \\mathbb{R}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "这个激活函数，会将输出层的神经元的输出值转换为一个 $(0, 1)$ 区间内的数。\n",
    "\n",
    "因为是二分类问题，我们设类别为0和1，我们将输出值大于0.5的样本分为1类，输出值小于0.5的类分为0类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "$$\n",
    "Z = XW + b\\\\\n",
    "\\hat{y} = \\sigma(Z)\n",
    "$$\n",
    "\n",
    "其中，$O \\in \\mathbb{R}^{n}$为输出层的结果，$\\sigma$为$\\rm sigmoid$激活函数。\n",
    "\n",
    "**注意：这里我们其实是做了广播，将$b$复制了$n-1$份后拼接成了维数为$n$的向量。**\n",
    "\n",
    "所以对数几率回归就可以写为：\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + e^{-XW + b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "使用对数损失函数，因为对数损失函数较其他损失函数有更好的性质，感兴趣的同学可以去查相关的资料。 \n",
    "\n",
    "针对二分类问题的对数损失函数：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = - y \\log{\\hat{y}} - (1 - y) \\log{(1 - \\hat{y})}\n",
    "$$\n",
    "\n",
    "在这个对数几率回归中，我们的损失函数对所有样本取个平均值：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = - \\frac{1}{n} \\sum^n_{i = 1}[y_i \\log{\\hat{y_i}} + (1 - y_i) \\log{(1 - \\hat{y_i})}]\n",
    "$$\n",
    "\n",
    "**注意，这里我们的提到的$\\log$均为$\\ln$，在numpy中为**`np.log`。\n",
    "\n",
    "因为我们的类别只有0和1，所以在这个对数损失函数中，要么前一项为0，要么后一项为0。\n",
    "\n",
    "如果当前样本的类别为0，那么前一项就为0，损失函数变为 $- \\log{(1 - \\hat{y})}$ ，因为我们的预测值 $0 < \\hat{y} < 1$ ，所以 $0 < 1 - \\hat{y} < 1$ ，$- \\log{(1 - \\hat{y})} > 0$ ，为了降低损失值，模型需要让预测值 $\\hat{y}$不断地趋于0。\n",
    "\n",
    "同理，如果当前样本的类别为1，那么降低损失值就可以使模型的预测值趋于1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数更新\n",
    "\n",
    "求得损失函数对参数的偏导数后，我们就可以使用**梯度下降**进行参数更新：\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\\\\\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。\n",
    "\n",
    "经过**一定次数**的迭代后，参数会收敛至最优点。这种基于梯度的优化算法很常用，训练神经网络主要使用这类优化算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "我们使用梯度下降更新参数$W$和$b$。为此需要求得损失函数对参数$W$和$b$的偏导数，根据链式法则有：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这里我们一项一项求，先求第一项：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} = - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第二项：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y}}{\\partial Z} & = \\frac{\\partial (\\frac{1}{1 + e^{-Z}})}{\\partial Z}\\\\\n",
    "& = \\frac{e^{-Z}}{(1 + e^{-Z})^2}\\\\\n",
    "& = \\frac{e^{-Z}}{(1 + e^{-Z})} \\frac{1}{(1 + e^{-Z})}\\\\\n",
    "& = \\frac{e^{-Z}}{(1 + e^{-Z})} (1 - \\frac{e^{-Z}}{(1 + e^{-Z})})\\\\\n",
    "& = \\sigma(Z)(1 - \\sigma(Z))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第三项：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial W} = X^{\\mathrm{T}}\n",
    "$$\n",
    "\n",
    "综上：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\sigma(Z_i)(1 - \\sigma(Z_i))] {X_i}^{\\mathrm{T}}\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\hat{y_i}(1 - \\hat{y_i})] {X_i}^{\\mathrm{T}}\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} [y_i(1 - \\hat{y_i}) - \\hat{y_i}(1 - y_i)] {X_i}^{\\mathrm{T}}\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} (y_i - y_i \\hat{y_i} - \\hat{y_i} + y_i \\hat{y_i}) {X_i}^{\\mathrm{T}}\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} (y_i - \\hat{y_i}) {X_i}^{\\mathrm{T}}\\\\\n",
    "&= \\frac{1}{n} [X^{\\mathrm{T}}(\\hat{y} - y)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "同理，求$\\rm loss$对$b$的偏导数：\n",
    "\n",
    "**注意，由于$b$是被广播成$n \\times K$的矩阵，因此实际上$b$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} &= \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\sigma(Z_i)(1 - \\sigma(Z_i))]\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\hat{y_i}(1 - \\hat{y_i})]\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} [y_i(1 - \\hat{y_i}) - \\hat{y_i}(1 - y_i)]\\\\\n",
    "&= - \\frac{1}{n} \\sum^n_{i = 1} (y_i - y_i \\hat{y_i} - \\hat{y_i} + y_i \\hat{y_i})\\\\\n",
    "&= \\frac{1}{n} \\sum^n_{i = 1} (\\hat{y_i} - y_i)\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "我们实现一个最简单的三层感知机，一个输入层，一个隐藏层，一个输出层，隐藏层单元个数为$h$个，输出层有$K$个单元。\n",
    "\n",
    "1. 我们将第一层的输入，定义为$X \\in \\mathbb{R}^{n \\times m}$，n个样本，m个特征。  \n",
    "2. 输入层到隐藏层之间的权重(weight)与偏置(bias)，分别为$W_1 \\in \\mathbb{R}^{m \\times h}$，$b_1 \\in \\mathbb{R}^{1 \\times h}$。  \n",
    "3. 隐藏层到输出层的权重和偏置分为别$W_2 \\in \\mathbb{R}^{h \\times K}$，$b_2 \\in \\mathbb{R}^{1 \\times K}$。\n",
    "\n",
    "隐藏层的激活函数选用ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max (0, x)\n",
    "$$\n",
    "\n",
    "我们用$H_1$表示第一个隐藏层的输出值，$O$表示输出层的输出值，这样，前向传播即可定义为\n",
    "\n",
    "$$\n",
    "Z = XW_1 + b_1\\\\\n",
    "H_1 = \\mathrm{ReLU}(Z)\\\\\n",
    "O = H_1 W_2 + b_2\n",
    "$$\n",
    "\n",
    "其中，$H_1 \\in \\mathbb{R}^{n \\times h}$，$O \\in \\mathbb{R}^{n \\times K}$。\n",
    "\n",
    "**注意：这里我们其实是做了广播，将$b_1$复制了$n-1$份后拼接成了维数为$n \\times h$的矩阵，同理，$b_2$也做了广播，拼成了$n \\times K$的矩阵。**\n",
    "\n",
    "最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y_i} & = \\mathrm{softmax}(O_i)\\\\\n",
    "& = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\hat{y_i}$表示第$i$类的概率值，也就是输出层第$i$个神经元经$\\mathrm{softmax}$激活后的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数使用交叉熵损失函数：\n",
    "$$\\mathrm{cross\\_entropy}(y, \\hat{y}) = -\\sum^{K}_{k=1}y_k \\log{(\\hat{y_k})}$$\n",
    "\n",
    "这样，$n$个样本的平均损失为：\n",
    "$$\n",
    "\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\n",
    "$$\n",
    "\n",
    "**注意，这里我们的提到的$\\log$均为$\\ln$，在numpy中为**`np.log`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "我们使用梯度下降训练模型，求解方式就是求出损失函数对参数的偏导数，即参数的梯度，然后将参数减去梯度乘以学习率，进行参数的更新。\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "其中，$\\alpha$是学习率。\n",
    "\n",
    "在这道题中，交叉熵损失函数的求导比较麻烦，我们先求神经网络的输出层的偏导数，写成链式法则的形式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\n",
    "$$\n",
    "\n",
    "首先求解第一项：\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}\n",
    "$$\n",
    "\n",
    "然后求解第二项，因为$\\hat{y_k}$的分母是$\\sum_k \\exp{(O_k)}$，里面包含$O_i$，所以每一个$\\hat{y_k}$的分母都包含$O_i$，这就要求反向传播的时候需要考虑这$K$项，将这$K$项的偏导数加在一起。\n",
    "\n",
    "这$K$项分别为：$\\frac{\\exp{(O_1)}}{\\sum_k \\exp{(O_k)}}$，$\\frac{\\exp{(O_2)}}{\\sum_k \\exp{(O_k)}}$，...，$\\frac{\\exp{(O_i)}}{\\sum_k \\exp{(O_k)}}$，...，$\\frac{\\exp{(O_k)}}{\\sum_k \\exp{(O_k)}}$。\n",
    "\n",
    "显然，这里只有分子带有$O_i$的这项与其他的项不同，因为分子和分母同时包含了$O_i$，而其他的项只有分母包含了$O_i$。\n",
    "\n",
    "这就需要在求解$\\frac{\\partial \\hat{y}}{\\partial O_i}$的时候分两种情况讨论\n",
    "1. 分子带$O_i$\n",
    "2. 分子不带$O_i$\n",
    "\n",
    "第一种情况，当分子含有$O_i$时：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y_i}}{\\partial O_i} & = \\frac{\\partial \\hat{y_i}}{\\partial O_i}\\\\\n",
    "& = \\frac{\\exp{(O_i)} (\\sum^{K}_{k=1} \\exp{(O_k)}) - (\\exp{(O_i)})^2 }{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\\n",
    "& = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}} \\frac{\\sum^{K}_{k=1} \\exp{(O_k)} - \\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\\\\\n",
    "& = \\hat{y_i} ( 1 - \\hat{y_i} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第二种情况，当分子不含$O_i$时，我们用$j$表示当前项的下标：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y_j}}{\\partial O_i} & = \\frac{- \\exp{(O_j)} \\exp{(O_i)}}{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\\n",
    "& = - \\hat{y_j} \\hat{y_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这样，$\\mathrm{loss}$对$O_i$的偏导数即为：\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\\n",
    "& = (- \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}) \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\\n",
    "& = - \\frac{1}{n} \\sum_n (y_i \\frac{1}{\\hat{y_i}} \\hat{y_i} ( 1 - \\hat{y_i} ) + \\sum^K_{k \\not= i} y_k \\frac{1}{\\hat{y_k}}( - \\hat{y_k} \\hat{y_i}))\\\\\n",
    "& = - \\frac{1}{n} \\sum_n ( y_i - y_i \\hat{y_i} - \\sum^K_{k \\not= i} y_k \\hat{y_i})\\\\\n",
    "& = - \\frac{1}{n} \\sum_n ( y_i  - \\hat{y_i} \\sum^K_{k = 1} y_k )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于我们处理的多类分类任务，一个样本只对应一个标记，所以$\\sum^K_{k = 1} y_k = 1$，上式在这种问题中，即可化简为：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} &= - \\frac{1}{n} \\sum_n ( y_i  - \\hat{y_i})\\\\\n",
    "& = \\frac{1}{n} \\sum_n (\\hat{y_i} -  y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "将其写成矩阵表达式：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O} &= \\frac{1}{n} (\\hat{y} - y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "也就是说，我们的损失函数对输出层的$K$个神经单元的偏导数为$\\mathrm{softmax}$激活值减去真值。\n",
    "\n",
    "接下来我们需要求损失函数对参数$W_2$和$b_2$的偏导数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\frac{\\partial loss}{\\partial W_2} \\in \\mathbb{R}^{h \\times K}$，$\\frac{\\partial loss}{\\partial b_2} \\in \\mathbb{R}^{1 \\times K}$。  \n",
    "**注意，由于$b_2$是被广播成$n \\times K$的矩阵，因此实际上$b_2$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。**\n",
    "\n",
    "同理，我们可以求得$\\mathrm{loss}$对$W_1$和$b_1$的偏导数：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\\n",
    "& = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于我们使用的是$\\mathrm{ReLU}$激活函数，它的偏导数为：\n",
    "\n",
    "$$\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } x < 0\\\\\n",
    "1 & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "所以上式为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial {W_1}_{ij}} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } {Z}_{ij} < 0\\\\\n",
    "    \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，${W_1}_{ij}$表示矩阵$W_1$第$i$行第$j$列的值，${Z}_{ij}$表示矩阵$Z$第$i$行第$j$列的值。  \n",
    "同理：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\\n",
    "& = \\begin{cases}\n",
    "0 &\\text{if } {Z}_{ij} < 0\\\\\n",
    "\\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\frac{\\partial loss}{\\partial W_1} \\in \\mathbb{R}^{m \\times h}$，$\\frac{\\partial loss}{\\partial b_1} \\in \\mathbb{R}^{1 \\times h}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数更新\n",
    "\n",
    "求得损失函数对四个参数的偏导数后，我们就可以使用梯度下降进行参数更新：\n",
    "$$\n",
    "W_2 := W_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_2}\\\\\n",
    "b_2 := b_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_2}\\\\\n",
    "W_1 := W_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_1}\\\\\n",
    "b_1 := b_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_1}\\\\\n",
    "$$\n",
    "其中，$\\alpha$是学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
