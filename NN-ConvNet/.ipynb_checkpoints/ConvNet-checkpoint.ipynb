{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# convolution net model:\n",
    "## layers i:\n",
    "layers shape: [width, height, channel_i]\n",
    "\n",
    "\n",
    "## kernels i:\n",
    "+ convolution:\n",
    "    kernels shape: [width, height, channel_(i-1), channel_i]\n",
    "    forward: Z^[i] = W^[i] * Z^[i-1], \n",
    "    backward: dW^[i] += dZ^[i] * Z^[i-1], dZ^[i-1] += W^[i] * dZ^[i]\n",
    "    \n",
    "+ pooling:\n",
    "    max        \n",
    "    mean\n",
    "             \n",
    "+ full connected layers:\n",
    "    \n",
    "\n",
    "\n",
    "+ input layer 0: [width, height, 3(rgb)]\n",
    "\n",
    "+ convolution output layer:\n",
    "\n",
    "## Programming Parameters\n",
    "+ kernels: \n",
    "    convolution: (layers, kernels, kernel_width, kernel_height, channel_i-1)\n",
    "    pooling: max, mean\n",
    "+ padding\n",
    "+ step\n",
    "+ layers: (layers, layer_width, layer_height, channel)\n",
    "+ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('./../data/', train=True, download=False,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                       torchvision.transforms.ToTensor(),\n",
    "                                       torchvision.transforms.Normalize(\n",
    "                                           (0.1307,), (0.3081,))\n",
    "                                   ])),\n",
    "        batch_size=128, shuffle=True)\n",
    "def getXY():\n",
    "    train_batch = enumerate(train_loader)\n",
    "    batch_idx, (train_imgs, train_labels) = next(train_batch)\n",
    "    return np.array(train_imgs), np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28)\n",
      "(128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "x, y = getXY()\n",
    "img_array = x\n",
    "print(img_array.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(img_array[2,0,:,:]*255)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W_b(n_c, n_c_pre, f_h, f_w):\n",
    "    W = np.random.rand(n_c, n_c_pre, f_h, f_w)\n",
    "    b = np.random.rand(n_c, 1, 1, 1)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = Z.copy()\n",
    "    A[A < 0] = 0\n",
    "    return A\n",
    "\n",
    "def relu_(Z, dA):\n",
    "    dZ = dA.copy()\n",
    "    dZ[Z<0] = 0\n",
    "    return dZ\n",
    "\n",
    "softmax_layer_sum = 0\n",
    "def softmax(Z):\n",
    "    Z = Z - Z.max()\n",
    "    A = np.exp(Z)\n",
    "    softmax_layer_sum = A.sum()\n",
    "    return A\n",
    "\n",
    "def softmax_(Z):\n",
    "    ex = np.exp(Z)\n",
    "    return ex*(softmax_layer_sum-ex)/softmax_layer_sum**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(slice, mode):\n",
    "    if mode==\"max\":\n",
    "        mask = x == np.max(x)\n",
    "    elif mode==\"mean\":\n",
    "        mask = np.ones(slice.shape)\n",
    "        mask = mask/mask.sum()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, stride, pad, mode=\"conv\"):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "    W -- Weights, numpy array of shape (n_C, n_C_prev, f, f)\n",
    "    b -- Biases, numpy array of shape (n_C, 1, 1, 1)\n",
    "    stride --\n",
    "    pad --\n",
    "\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_C, n_H, n_W)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "    # get shape\n",
    "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "    (n_C, n_C_prev, f, f) = W.shape\n",
    "\n",
    "    # create container Z and padded A_prev\n",
    "    n_H = int(np.floor((n_H_prev + 2 * pad - f) / stride + 1))\n",
    "    n_W = int(np.floor((n_W_prev + 2 * pad - f) / stride + 1))\n",
    "    Z = np.zeros((m, n_C, n_H, n_W))\n",
    "    A = np.zeros((m, n_C, n_H, n_W))\n",
    "    A_prev = np.pad(A_prev, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=(0))\n",
    "\n",
    "    # propagation\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n_C):\n",
    "            for k in range(0, n_H):\n",
    "                for l in range(0, n_W):\n",
    "                    assert (k*stride+f-1 < n_H_prev+2*pad and l*stride+f-1 <= n_W_prev+2*pad)\n",
    "                    A_pre_slice = A_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f]\n",
    "                    conv = []\n",
    "                    if mode == \"conv\":\n",
    "                        conv = A_pre_slice * W[j] + b[j]\n",
    "                    elif mode == \"max\" or mode == \"mean\":\n",
    "                        mask = get_mask(A_pre_slice, mode)\n",
    "                        conv = A_pre_slice * mask\n",
    "                    Z[i, j, k, l] += conv.sum()\n",
    "\n",
    "    assert (Z.shape == (m, n_C, n_H, n_W))\n",
    "    A = relu(Z)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dA, Z, W, b, A_prev, stride, pad):\n",
    "    \n",
    "    # get shape\n",
    "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "    (n_C, n_C_prev, f, f) = W.shape\n",
    "    (m, n_C, n_H, n_W) = Z.shape\n",
    "    \n",
    "    # set gradient container\n",
    "    dZ = relu_(Z, dA)\n",
    "    dW = np.zeros((n_C, n_C_prev, f, f))\n",
    "    db = np.zeros((n_C, 1, 1, 1))\n",
    "    A_prev = np.pad(A_prev, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=(0))\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    # propagation\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n_C):\n",
    "            for k in range(0, n_H):\n",
    "                for l in range(0, n_W):\n",
    "                    assert (k*stride+f-1 < n_H_prev+2*pad and l*stride+f-1 <= n_W_prev+2*pad)\n",
    "                    A_pre_slice = A_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f]\n",
    "                    dW, db, dA_prev = [],[],[]\n",
    "                    if mode == \"conv\":\n",
    "                        dW[j] += dZ[i, j, k, l] * A_pre_slice\n",
    "                        db[j] += dZ[i, j, k, l]\n",
    "                        dA_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f] += W[j] * dZ[i, j, k, l]\n",
    "                    elif mode == \"max\" or mode == \"mean\":\n",
    "                        mask = get_mask(A_pre_slice, mode)\n",
    "                        dA_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f] += mask * dZ[i, j, k, l]  \n",
    "    \n",
    "    return dW, db, dA_prev[:, :, pad:pad+n_H_prev, pad:pad+n_W_prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward(A_prev, W, b, g):\n",
    "    Z = np.dot(W, A_prec) + b\n",
    "    A = g(Z)\n",
    "    return Z, A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(A_prev, Z, g_, dA):\n",
    "    m, n_l, n_l_prev= Z.shape\n",
    "    \n",
    "    dZ = g_(Z)*dA\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = dA.sum(axis=1, keepdim=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_lose(A, y):\n",
    "    assert(A.shape == y.shape)\n",
    "    return -y*np.log(A)\n",
    "def cross_lose_(A, y):\n",
    "    return -1/A*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -0.          0.35667494]\n",
      "[-0.         -0.         -1.42857143]\n",
      "[ 0.          0.         -0.50953563]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.1,0.2,0.7])\n",
    "y = np.array([0,0,1])\n",
    "l = cross_lose(a, y)\n",
    "l_ = cross_lose_(a, y)\n",
    "print(l)\n",
    "print(l_)\n",
    "print(l*l_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batchs, itrs, learning_rate):\n",
    "    los = []\n",
    "    \n",
    "    # model data\n",
    "    (A1, Z1, pad1, stride1, mode1, (W1, b1)) = ([], [], 1, 1, \"conv\", get_W_b(16, 1, 3, 3))\n",
    "    (A2, Z2, pad2, stride2, mode2, (W2, b2)) = ([], [], 1, 2, \"conv\", get_W_b(16, 16, 3, 3))\n",
    "    (A3, Z3, pad3, stride3, mode3, (W3, b3)) = ([], [], 1, 2, \"max\", get_W_b(0, 0, 0, 0))\n",
    "    (A4, Z4, pad4, stride4, mode4, (W4, b4)) = ([], [], 0, 2, \"conv\", get_W_b(16, 16, 3, 3))\n",
    "    (A5, Z5, pad5, stride5, mode5, (W5, b5)) = ([], [], 0, 1, \"conv\", get_W_b(32, 16, 3, 3))\n",
    "    W6 = np.random.rand(16, 32)\n",
    "    b6 = np.random.rand(16)\n",
    "    W7 = np.random.rand(10, 16)\n",
    "    b7 = np.random.rand(10)\n",
    "    \n",
    "    for i in range(0, batchs):\n",
    "        A_0, y = getXY()\n",
    "        for itr in range(0, itrs):\n",
    "        # forward propagation\n",
    "            # conv\n",
    "            Z1, A1 = conv_forward(A_0, W1, b1, 1, 1, \"conv\")\n",
    "            Z2, A2 = conv_forward(A_1, W2, b2, 1, 2, \"conv\")\n",
    "            Z3, A3 = conv_forward(A_2, W3, b3, 1, 2, \"max\")\n",
    "            Z4, A4 = conv_forward(A_3, W4, b4, 1, 2, \"conv\")\n",
    "            Z5, A5 = conv_forward(A_4, W5, b5, 1, 1, \"conv\")\n",
    "            \n",
    "            # conv to fc\n",
    "            m, A5_c, A5_h, A5_w = A5.shape \n",
    "            A5_ = A5.flatten('C')\n",
    "            A5_ = A5_.reshape((m, -1), order='C').T\n",
    "            \n",
    "            # fc\n",
    "            Z6, A6 = fc_forward(A5_, W6, b6, relu)\n",
    "            Z7, A7 = fc_forward(A6, W7, b7, softmax)\n",
    "            \n",
    "        #lose\n",
    "            l = cross_lose(A7, y)\n",
    "            los.append(l)\n",
    "            \n",
    "        # backward propagation\n",
    "            # fc\n",
    "            dA7 = l * cross_lose_(A7, y)\n",
    "            dW7, db7, dA6 = fc_backward(A6, Z7, softmax, dA7)\n",
    "            dW6, db6, dA5_ = fc_backward(A5, Z6, relu, dA6)\n",
    "            \n",
    "            # fc to conv\n",
    "            dA5 = dA5_.reshape(A5.shape, order='C')\n",
    "            \n",
    "            # conv\n",
    "            dW5, db5 dA4 = conv_backward(dA5, Z5, W5, b5, A4, stride5, pad5, mode=\"conv\")\n",
    "            dW4, db4 dA3 = conv_backward(dA4, Z4, b4, A3, stride4, pad4, mode=\"conv\")\n",
    "            dW3, db3 dA2 = conv_backward(dA5, Z3, b3, A2, stride3, pad3, mode=\"max\")\n",
    "            dW2, db2 dA1 = conv_backward(dA5, Z3, b2, A1, stride2, pad2, mode=\"conv\")\n",
    "            dW1, db1 dA0 = conv_backward(dA5, Z1, b1, A0, stride1, pad1, mode=\"conv\")\n",
    "            \n",
    "        # refresh the weights\n",
    "            W7 = W7 - learning_rate*dW7\n",
    "            b7 = b7 - learning_rate*b7\n",
    "            W6 = W6 - learning_rate*dW6\n",
    "            b6 = b6 - learning_rate*b6\n",
    "            W5 = W5 - learning_rate*dW5\n",
    "            b5 = b5 - learning_rate*b5\n",
    "            W4 = W4 - learning_rate*dW4\n",
    "            b4 = b4 - learning_rate*b4\n",
    "            W3 = W3 - learning_rate*dW3\n",
    "            b3 = b3 - learning_rate*b3\n",
    "            W2 = W2 - learning_rate*dW2\n",
    "            b2 = b2 - learning_rate*b2\n",
    "            W1 = W1 - learning_rate*dW1\n",
    "            b1 = b1 - learning_rate*b1\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(slice, mode):\n",
    "    if mode==\"max\":\n",
    "        mask = slice == np.max(slice)\n",
    "    elif mode==\"mean\":\n",
    "        mask = np.ones(slice.shape)\n",
    "        mask = mask/np.sum(mask, axis=(1,2))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,0]\n",
    "    ],\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9]\n",
    "    ]\n",
    "])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[45]]\n",
      "\n",
      " [[45]]]\n",
      "(2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "b = np.sum(a, axis=(1,2), keepdims=True)\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.02222222, 0.04444444, 0.06666667],\n",
       "        [0.08888889, 0.11111111, 0.13333333],\n",
       "        [0.15555556, 0.17777778, 0.2       ]],\n",
       "\n",
       "       [[0.02222222, 0.04444444, 0.06666667],\n",
       "        [0.08888889, 0.11111111, 0.13333333],\n",
       "        [0.15555556, 0.17777778, 0.2       ]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False,  True, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False,  True]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == np.max(a, axis=(1,2),keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
