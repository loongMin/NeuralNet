{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# convolution net model:\n",
    "## layers i:\n",
    "layers shape: [width, height, channel_i]\n",
    "\n",
    "\n",
    "## kernels i:\n",
    "+ convolution:\n",
    "    kernels shape: [width, height, channel_(i-1), channel_i]\n",
    "    forward: Z^[i] = W^[i] * Z^[i-1], \n",
    "    backward: dW^[i] += dZ^[i] * Z^[i-1], dZ^[i-1] += W^[i] * dZ^[i]\n",
    "    \n",
    "+ pooling:\n",
    "    max        \n",
    "    mean\n",
    "             \n",
    "+ full connected layers:\n",
    "    \n",
    "\n",
    "\n",
    "+ input layer 0: [width, height, 3(rgb)]\n",
    "\n",
    "+ convolution output layer:\n",
    "\n",
    "## Programming Parameters\n",
    "+ kernels: \n",
    "    convolution: (layers, kernels, kernel_width, kernel_height, channel_i-1)\n",
    "    pooling: max, mean\n",
    "+ padding\n",
    "+ step\n",
    "+ layers: (layers, layer_width, layer_height, channel)\n",
    "+ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('./../data/', train=True, download=False,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                       torchvision.transforms.ToTensor(),\n",
    "                                       torchvision.transforms.Normalize(\n",
    "                                           (0.1307,), (0.3081,))\n",
    "                                   ])),\n",
    "        batch_size=128, shuffle=True)\n",
    "def getXY():\n",
    "    train_batch = enumerate(train_loader)\n",
    "    batch_idx, (train_imgs, train_labels) = next(train_batch)\n",
    "    return np.array(train_imgs), np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28)\n",
      "(128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "x, y = getXY()\n",
    "img_array = x\n",
    "print(img_array.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(img_array[2,0,:,:]*255)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W_b(n_c, n_c_pre, f_h, f_w):\n",
    "    W = np.random.rand(n_c, n_c_pre, f_h, f_w)\n",
    "    b = np.random.rand(n_c, 1, 1, 1)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = Z.copy()\n",
    "    A[A < 0] = 0\n",
    "    return A\n",
    "\n",
    "def relu_(Z, dA):\n",
    "    dZ = dA.copy()\n",
    "    dZ[Z<0] = 0\n",
    "    return dZ\n",
    "\n",
    "softmax_layer_sum = 0\n",
    "def softmax(Z):\n",
    "    Z = Z - Z.max()\n",
    "    A = np.exp(Z)\n",
    "    softmax_layer_sum = A.sum()\n",
    "    return A\n",
    "\n",
    "def softmax_(Z):\n",
    "    ex = np.exp(Z)\n",
    "    return ex*(softmax_layer_sum-ex)/softmax_layer_sum**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(slice, mode):\n",
    "    if mode==\"max\":\n",
    "        mask = x == np.max(x)\n",
    "    elif mode==\"mean\":\n",
    "        mask = np.ones(slice.shape)\n",
    "        mask = mask/mask.sum()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, stride, pad, mode=\"conv\"):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "    W -- Weights, numpy array of shape (n_C, n_C_prev, f, f)\n",
    "    b -- Biases, numpy array of shape (n_C, 1, 1, 1)\n",
    "    stride --\n",
    "    pad --\n",
    "\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_C, n_H, n_W)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "    # get shape\n",
    "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "    (n_C, n_C_prev, f, f) = W.shape\n",
    "\n",
    "    # create container Z and padded A_prev\n",
    "    n_H = int(np.floor((n_H_prev + 2 * pad - f) / stride + 1))\n",
    "    n_W = int(np.floor((n_W_prev + 2 * pad - f) / stride + 1))\n",
    "    Z = np.zeros((m, n_C, n_H, n_W))\n",
    "    A = np.zeros((m, n_C, n_H, n_W))\n",
    "    A_prev = np.pad(A_prev, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=(0))\n",
    "\n",
    "    # propagation\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n_C):\n",
    "            for k in range(0, n_H):\n",
    "                for l in range(0, n_W):\n",
    "                    assert (k*stride+f-1 < n_H_prev+2*pad and l*stride+f-1 <= n_W_prev+2*pad)\n",
    "                    A_pre_slice = A_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f]\n",
    "                    conv = []\n",
    "                    if mode == \"conv\":\n",
    "                        conv = A_pre_slice * W[j] + b[j]\n",
    "                    elif mode == \"max\" or mode == \"mean\":\n",
    "                        mask = get_mask(A_pre_slice, mode)\n",
    "                        conv = A_pre_slice * mask\n",
    "                    Z[i, j, k, l] += conv.sum()\n",
    "\n",
    "    assert (Z.shape == (m, n_C, n_H, n_W))\n",
    "    A = relu(Z)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward(A_prev, W, b, g):\n",
    "    Z = np.dot(W, A_prec) + b\n",
    "    A = g(Z)\n",
    "    return Z, A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lose(A, y):\n",
    "    assert(A.shape == y.shape)\n",
    "    return y*np.log(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(dA, A_prev, Z, g_):\n",
    "    m, n_l, n_l_prev= Z.shape\n",
    "    \n",
    "    dZ = g_(Z)*dA\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = dA.sum(axis=1, keepdim=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dA, Z, W, b, A_prev, stride, pad):\n",
    "    \n",
    "    # get shape\n",
    "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "    (n_C, n_C_prev, f, f) = W.shape\n",
    "    (m, n_C, n_H, n_W) = Z.shape\n",
    "    \n",
    "    # set gradient container\n",
    "    dZ = relu_(Z, dA)\n",
    "    dW = np.zeros((n_C, n_C_prev, f, f))\n",
    "    db = np.zeros((n_C, 1, 1, 1))\n",
    "    dA_prev = np.zeros((m, n_C_prev, n_H_prev, n_W_prev))\n",
    "    \n",
    "    # propagation\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n_C):\n",
    "            for k in range(0, n_H):\n",
    "                for l in range(0, n_W):\n",
    "                    assert (k*stride+f-1 < n_H_prev+2*pad and l*stride+f-1 <= n_W_prev+2*pad)\n",
    "                    A_pre_slice = A_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f]\n",
    "                    dW, db, dA_prev = [],[],[]\n",
    "                    if mode == \"conv\":\n",
    "                        dW[j] += dZ[i, j, k, l] * A_pre_slice\n",
    "                        db[j] += dZ[i, j, k, l]\n",
    "                        dA_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f] += W[j] * dZ[i, j, k, l]\n",
    "                    elif mode == \"max\" or mode == \"mean\":\n",
    "                        mask = get_mask(A_pre_slice, mode)\n",
    "                        dA_prev[i, :, k*stride:k*stride+f, l*stride:l*stride+f] += mask * dZ[i, j, k, l]                   \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batchs, itrs, learning_rate):\n",
    "    # model data\n",
    "    (A1, Z1, pad1, step1, mode1, (W1, b1)) = ([], [], 1, 1, \"conv\", get_W_b(16, 1, 3, 3))\n",
    "    (A2, Z2, pad2, step2, mode2, (W2, b2)) = ([], [], 1, 2, \"conv\", get_W_b(16, 16, 3, 3))\n",
    "    (A3, Z3, pad3, step3, mode3, (W3, b3)) = ([], [], 1, 2, \"max\", get_W_b(0, 0, 0, 0))\n",
    "    (A4, Z4, pad4, step4, mode4, (W4, b4)) = ([], [], 0, 2, \"conv\", get_W_b(16, 16, 3, 3))\n",
    "    (A5, Z5, pad5, step5, mode5, (W5, b5)) = ([], [], 0, 1, \"conv\", get_W_b(32, 16, 3, 3))\n",
    "    W6 = np.random.rand(16, 32)\n",
    "    b6 = np.random.rand(16)\n",
    "    W7 = np.random.rand(10, 16)\n",
    "    b7 = np.random.rand(10)\n",
    "    \n",
    "    for i in range(0, batchs):\n",
    "        A_0, y = getXY()\n",
    "        for itr in range(0, itrs):\n",
    "            # forward propagation\n",
    "            Z1, A1 = conv_forward(A_0, W1, b1, 1, 1, \"conv\")\n",
    "            Z2, A2 = conv_forward(A_1, W2, b2, 1, 2, \"conv\")\n",
    "            Z3, A3 = conv_forward(A_2, W3, b3, 1, 2, \"max\")\n",
    "            Z4, A4 = conv_forward(A_3, W4, b4, 1, 2, \"conv\")\n",
    "            Z5, A5 = conv_forward(A_4, W5, b5, 1, 1, \"conv\")\n",
    "            \n",
    "            A5_ = np.squeeze(A5, axis=(2, 3)).T\n",
    "            \n",
    "            Z6, A6 = fc_forward(A5_, W6, b6, relu)\n",
    "            Z7, A7 = fc_forward(A6, W7, b7, softmax)\n",
    "            \n",
    "            #lose\n",
    "            los = lose(A7, y)\n",
    "            \n",
    "            # backward propagation\n",
    "            dZ7 = A7 - 1\n",
    "            \n",
    "            \n",
    "    \n",
    "    (A_prev, W, b, stride, pad, mode=\"conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 4, 1)\n",
      "(1, 4)\n",
      "[[1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [\n",
    "        [[1],[2],[3],[4]]\n",
    "    ]\n",
    "])\n",
    "print(a.shape)\n",
    "b = np.squeeze(a, axis=(0,1)).T\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
